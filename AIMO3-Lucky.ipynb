{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7973856c",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-02-18T13:13:47.902939Z",
     "iopub.status.busy": "2026-02-18T13:13:47.902508Z",
     "iopub.status.idle": "2026-02-18T13:14:53.094136Z",
     "shell.execute_reply": "2026-02-18T13:14:53.093675Z"
    },
    "papermill": {
     "duration": 65.196463,
     "end_time": "2026-02-18T13:14:53.095039",
     "exception": false,
     "start_time": "2026-02-18T13:13:47.898576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: keras 3.10.0\r\n",
      "Uninstalling keras-3.10.0:\r\n",
      "  Successfully uninstalled keras-3.10.0\r\n",
      "Found existing installation: matplotlib 3.10.0\r\n",
      "Uninstalling matplotlib-3.10.0:\r\n",
      "  Successfully uninstalled matplotlib-3.10.0\r\n",
      "Found existing installation: scikit-learn 1.6.1\r\n",
      "Uninstalling scikit-learn-1.6.1:\r\n",
      "  Successfully uninstalled scikit-learn-1.6.1\r\n",
      "Found existing installation: tensorflow 2.19.0\r\n",
      "Uninstalling tensorflow-2.19.0:\r\n",
      "  Successfully uninstalled tensorflow-2.19.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bf5c2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:14:53.100990Z",
     "iopub.status.busy": "2026-02-18T13:14:53.100810Z",
     "iopub.status.idle": "2026-02-18T13:14:53.103560Z",
     "shell.execute_reply": "2026-02-18T13:14:53.103173Z"
    },
    "papermill": {
     "duration": 0.006573,
     "end_time": "2026-02-18T13:14:53.104270",
     "exception": false,
     "start_time": "2026-02-18T13:14:53.097697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7764dbf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:14:53.109363Z",
     "iopub.status.busy": "2026-02-18T13:14:53.109216Z",
     "iopub.status.idle": "2026-02-18T13:14:53.111408Z",
     "shell.execute_reply": "2026-02-18T13:14:53.111044Z"
    },
    "papermill": {
     "duration": 0.005655,
     "end_time": "2026-02-18T13:14:53.112119",
     "exception": false,
     "start_time": "2026-02-18T13:14:53.106464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3babfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:14:53.117181Z",
     "iopub.status.busy": "2026-02-18T13:14:53.117034Z",
     "iopub.status.idle": "2026-02-18T13:14:53.120161Z",
     "shell.execute_reply": "2026-02-18T13:14:53.119767Z"
    },
    "papermill": {
     "duration": 0.006557,
     "end_time": "2026-02-18T13:14:53.120889",
     "exception": false,
     "start_time": "2026-02-18T13:14:53.114332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_env(input_archive, temp_dir):\n",
    "\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n",
    "    \n",
    "    subprocess.run([\n",
    "        sys.executable, \n",
    "        '-m', \n",
    "        'pip', \n",
    "        'install', \n",
    "        '--no-index', \n",
    "        '--find-links', \n",
    "        f'{temp_dir}/wheels', \n",
    "        'unsloth', \n",
    "        'trl', \n",
    "        'vllm', \n",
    "        'openai_harmony'\n",
    "    ], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1db9018",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-02-18T13:14:53.125960Z",
     "iopub.status.busy": "2026-02-18T13:14:53.125829Z",
     "iopub.status.idle": "2026-02-18T13:18:04.430348Z",
     "shell.execute_reply": "2026-02-18T13:18:04.429934Z"
    },
    "papermill": {
     "duration": 191.312566,
     "end_time": "2026-02-18T13:18:04.435738",
     "exception": false,
     "start_time": "2026-02-18T13:14:53.123172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/tmp/setup/wheels\n",
      "Processing /kaggle/tmp/setup/wheels/unsloth-2025.12.9-py3-none-any.whl\n",
      "Processing /kaggle/tmp/setup/wheels/trl-0.24.0-py3-none-any.whl\n",
      "Processing /kaggle/tmp/setup/wheels/vllm-0.11.2-cp38-abi3-manylinux1_x86_64.whl\n",
      "Processing /kaggle/tmp/setup/wheels/openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Processing /kaggle/tmp/setup/wheels/unsloth_zoo-2025.12.7-py3-none-any.whl (from unsloth)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (26.0rc2)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
      "Processing /kaggle/tmp/setup/wheels/tyro-1.0.3-py3-none-any.whl (from unsloth)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
      "Processing /kaggle/tmp/setup/wheels/xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl (from unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (from unsloth)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
      "Processing /kaggle/tmp/setup/wheels/datasets-4.3.0-py3-none-any.whl (from unsloth)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.17.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm) (2025.11.3)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm) (5.5.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.32.5)\n",
      "Requirement already satisfied: blake3 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.0.8)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.22.1)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.123.10)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm) (3.13.3)\n",
      "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.15.0)\n",
      "Requirement already satisfied: pydantic>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.12.5)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm) (11.3.0)\n",
      "Processing /kaggle/tmp/setup/wheels/prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (from vllm)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.12.0)\n",
      "Processing /kaggle/tmp/setup/wheels/lm_format_enforcer-0.11.3-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/diskcache-5.6.3-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/lark-1.2.2-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.15.0)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (3.20.3)\n",
      "Processing /kaggle/tmp/setup/wheels/partial_json_parser-0.2.1.1.post7-py3-none-any.whl (from vllm)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (26.2.1)\n",
      "Processing /kaggle/tmp/setup/wheels/msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/gguf-0.17.1-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/mistral_common-1.8.8-py3-none-any.whl (from mistral_common[image]>=1.8.5->vllm)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.12.0.88)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm) (6.0.3)\n",
      "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.17.0)\n",
      "Processing /kaggle/tmp/setup/wheels/setuptools-80.9.0-py3-none-any.whl (from vllm)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm) (0.8.1)\n",
      "Processing /kaggle/tmp/setup/wheels/compressed_tensors-0.12.2-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/depyf-0.20.0-py3-none-any.whl (from vllm)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm) (3.1.1)\n",
      "Processing /kaggle/tmp/setup/wheels/watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\n",
      "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm) (4.0.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm) (1.15.3)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from vllm) (1.13.0)\n",
      "Processing /kaggle/tmp/setup/wheels/pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/anthropic-0.71.0-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/model_hosting_container_standards-0.1.12-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from vllm)\n",
      "Requirement already satisfied: ray>=2.48.0 in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (2.53.0)\n",
      "Processing /kaggle/tmp/setup/wheels/torch-2.9.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/torchaudio-2.9.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/torchvision-0.24.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/flashinfer_python-0.5.2-py3-none-any.whl (from vllm)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.17.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (1.3.1)\n",
      "Processing /kaggle/tmp/setup/wheels/loguru-0.7.3-py3-none-any.whl (from compressed-tensors==0.12.2->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/astor-0.8.1-py2.py3-none-any.whl (from depyf==0.20.0->vllm)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.20.0->vllm) (0.4.0)\n",
      "Processing /kaggle/tmp/setup/wheels/apache_tvm_ffi-0.1.7-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (8.3.1)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cudnn_frontend-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cutlass_dsl-4.3.4-cp312-cp312-manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (12.575.51)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (0.9.0)\n",
      "Processing /kaggle/tmp/setup/wheels/interegular-0.3.3-py37-none-any.whl (from lm-format-enforcer==0.11.3->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from numba==0.61.2->vllm)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2025.10.0)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from unsloth)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (22.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\n",
      "Processing /kaggle/tmp/setup/wheels/multiprocess-0.70.16-py312-none-any.whl (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/fsspec-2025.9.0-py3-none-any.whl (from torch>=2.4.0->unsloth)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.0.4)\n",
      "Processing /kaggle/tmp/setup/wheels/fastapi_cli-0.0.20-py3-none-any.whl (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.38.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.1rc0)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (4.25.1)\n",
      "Processing /kaggle/tmp/setup/wheels/pydantic_extra_types-2.10.6-py3-none-any.whl (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\n",
      "Requirement already satisfied: jmespath in /usr/local/lib/python3.12/dist-packages (from model-hosting-container-standards<1.0.0->vllm) (1.0.1)\n",
      "Processing /kaggle/tmp/setup/wheels/supervisor-4.3.0-py2.py3-none-any.whl (from model-hosting-container-standards<1.0.0->vllm)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (0.4.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.2)\n",
      "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (13.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2026.1.4)\n",
      "Processing /kaggle/tmp/setup/wheels/torchao-0.15.0+cu128-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from unsloth_zoo>=2025.12.7->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/cut_cross_entropy-25.1.1-py3-none-any.whl (from unsloth_zoo>=2025.12.7->unsloth)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.22.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.20.0)\n",
      "Processing /kaggle/tmp/setup/wheels/rich_toolkit-0.17.1-py3-none-any.whl (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/fastapi_cloud_cli-0.8.0-py3-none-any.whl (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.27.1)\n",
      "Processing /kaggle/tmp/setup/wheels/cuda_python-13.1.1-py3-none-any.whl (from nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/pycountry-24.6.1-py3-none-any.whl (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Processing /kaggle/tmp/setup/wheels/httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.1)\n",
      "Processing /kaggle/tmp/setup/wheels/uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
      "Processing /kaggle/tmp/setup/wheels/cuda_bindings-13.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/cuda_pathfinder-1.3.3-py3-none-any.whl (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.42.1)\n",
      "Processing /kaggle/tmp/setup/wheels/fastar-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Installing collected packages: torchao, supervisor, uvloop, triton, setuptools, setproctitle, rignore, pycountry, pybase64, partial-json-parser, outlines_core, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cudnn-frontend, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multiprocess, msgspec, loguru, llvmlite, llguidance, lark, interegular, httptools, gguf, fsspec, fastar, diskcache, cuda-pathfinder, cbor2, astor, apache-tvm-ffi, watchfiles, tyro, nvidia-cusparse-cu12, nvidia-cufft-cu12, numba, depyf, cuda-bindings, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai_harmony, nvidia-cusolver-cu12, lm-format-enforcer, cuda-python, anthropic, torch, nvidia-cutlass-dsl, model-hosting-container-standards, fastapi-cloud-cli, fastapi-cli, datasets, xgrammar, xformers, torchvision, torchaudio, mistral_common, flashinfer-python, cut_cross_entropy, compressed-tensors, bitsandbytes, trl, unsloth_zoo, vllm, unsloth\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.2.0\n",
      "    Uninstalling setuptools-75.2.0:\n",
      "      Successfully uninstalled setuptools-75.2.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvshmem-cu12\n",
      "    Found existing installation: nvidia-nvshmem-cu12 3.4.5\n",
      "    Uninstalling nvidia-nvshmem-cu12-3.4.5:\n",
      "      Successfully uninstalled nvidia-nvshmem-cu12-3.4.5\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.18\n",
      "    Uninstalling multiprocess-0.70.18:\n",
      "      Successfully uninstalled multiprocess-0.70.18\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.43.0\n",
      "    Uninstalling llvmlite-0.43.0:\n",
      "      Successfully uninstalled llvmlite-0.43.0\n",
      "  Attempting uninstall: lark\n",
      "    Found existing installation: lark 1.3.0\n",
      "    Uninstalling lark-1.3.0:\n",
      "      Successfully uninstalled lark-1.3.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.10.0\n",
      "    Uninstalling fsspec-2025.10.0:\n",
      "      Successfully uninstalled fsspec-2025.10.0\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.60.0\n",
      "    Uninstalling numba-0.60.0:\n",
      "      Successfully uninstalled numba-0.60.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: cuda-python\n",
      "    Found existing installation: cuda-python 12.6.2.post1\n",
      "    Uninstalling cuda-python-12.6.2.post1:\n",
      "      Successfully uninstalled cuda-python-12.6.2.post1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0+cu126\n",
      "    Uninstalling torch-2.8.0+cu126:\n",
      "      Successfully uninstalled torch-2.8.0+cu126\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.4.2\n",
      "    Uninstalling datasets-4.4.2:\n",
      "      Successfully uninstalled datasets-4.4.2\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.23.0+cu126\n",
      "    Uninstalling torchvision-0.23.0+cu126:\n",
      "      Successfully uninstalled torchvision-0.23.0+cu126\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.8.0+cu126\n",
      "    Uninstalling torchaudio-2.8.0+cu126:\n",
      "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
      "Successfully installed anthropic-0.71.0 apache-tvm-ffi-0.1.7 astor-0.8.1 bitsandbytes-0.49.0 cbor2-5.7.1 compressed-tensors-0.12.2 cuda-bindings-13.1.1 cuda-pathfinder-1.3.3 cuda-python-13.1.1 cut_cross_entropy-25.1.1 datasets-4.3.0 depyf-0.20.0 diskcache-5.6.3 fastapi-cli-0.0.20 fastapi-cloud-cli-0.8.0 fastar-0.8.0 flashinfer-python-0.5.2 fsspec-2025.9.0 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 lark-1.2.2 llguidance-1.3.0 llvmlite-0.44.0 lm-format-enforcer-0.11.3 loguru-0.7.3 mistral_common-1.8.8 model-hosting-container-standards-0.1.12 msgspec-0.20.0 multiprocess-0.70.16 numba-0.61.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-frontend-1.17.0 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cutlass-dsl-4.3.4 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 openai_harmony-0.0.8 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post7 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.3 pycountry-24.6.1 pydantic-extra-types-2.10.6 rich-toolkit-0.17.1 rignore-0.7.6 setproctitle-1.3.7 setuptools-80.9.0 supervisor-4.3.0 torch-2.9.0+cu128 torchao-0.15.0+cu128 torchaudio-2.9.0+cu128 torchvision-0.24.0+cu128 triton-3.5.0 trl-0.24.0 tyro-1.0.3 unsloth-2025.12.9 unsloth_zoo-2025.12.7 uvloop-0.22.1 vllm-0.11.2 watchfiles-1.1.1 xformers-0.0.33.post1 xgrammar-0.1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyldavis 3.4.1 requires scikit-learn>=1.0.0, which is not installed.\n",
      "ydata-profiling 4.18.1 requires matplotlib<=3.10,>=3.5, which is not installed.\n",
      "stable-baselines3 2.1.0 requires matplotlib, which is not installed.\n",
      "sentence-transformers 5.1.1 requires scikit-learn, which is not installed.\n",
      "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
      "cuml-cu12 25.6.0 requires scikit-learn>=1.5, which is not installed.\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "bigframes 2.26.0 requires matplotlib>=3.7.1, which is not installed.\n",
      "arviz 0.22.0 requires matplotlib>=3.8, which is not installed.\n",
      "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
      "shap 0.49.1 requires scikit-learn, which is not installed.\n",
      "fastai 2.8.4 requires matplotlib, which is not installed.\n",
      "fastai 2.8.4 requires scikit-learn, which is not installed.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, which is not installed.\n",
      "cudf-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\n",
      "cuml-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "pylibraft-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "cuvs-cu12 25.6.1 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "fastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\n",
      "fastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0+cu128 which is incompatible.\n",
      "rmm-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "set_env(\n",
    "    input_archive='/kaggle/input/notebooks/andreasbis/aimo-3-utils/wheels.tar.gz', \n",
    "    temp_dir='/kaggle/tmp/setup'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19647c5b",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-02-18T13:18:04.444778Z",
     "iopub.status.busy": "2026-02-18T13:18:04.444309Z",
     "iopub.status.idle": "2026-02-18T13:18:04.461620Z",
     "shell.execute_reply": "2026-02-18T13:18:04.461256Z"
    },
    "papermill": {
     "duration": 0.022735,
     "end_time": "2026-02-18T13:18:04.462464",
     "exception": false,
     "start_time": "2026-02-18T13:18:04.439729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl100k_base.tiktoken\n",
      "o200k_base.tiktoken\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ls', '/kaggle/tmp/setup/tiktoken_encodings'], returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81dceac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:18:04.470924Z",
     "iopub.status.busy": "2026-02-18T13:18:04.470788Z",
     "iopub.status.idle": "2026-02-18T13:18:04.473461Z",
     "shell.execute_reply": "2026-02-18T13:18:04.473137Z"
    },
    "papermill": {
     "duration": 0.007764,
     "end_time": "2026-02-18T13:18:04.474156",
     "exception": false,
     "start_time": "2026-02-18T13:18:04.466392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
    "os.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aed3311c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:18:04.483310Z",
     "iopub.status.busy": "2026-02-18T13:18:04.483168Z",
     "iopub.status.idle": "2026-02-18T13:18:11.947892Z",
     "shell.execute_reply": "2026-02-18T13:18:11.947423Z"
    },
    "papermill": {
     "duration": 7.470872,
     "end_time": "2026-02-18T13:18:11.949493",
     "exception": false,
     "start_time": "2026-02-18T13:18:04.478621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import contextlib\n",
    "from typing import Optional\n",
    "from jupyter_client import KernelManager\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from openai_harmony import (\n",
    "    HarmonyEncodingName, \n",
    "    load_harmony_encoding, \n",
    "    SystemContent, \n",
    "    ReasoningEffort, \n",
    "    ToolNamespaceConfig, \n",
    "    Author, \n",
    "    Message, \n",
    "    Role, \n",
    "    TextContent, \n",
    "    Conversation\n",
    ")\n",
    "\n",
    "from transformers import set_seed\n",
    "import kaggle_evaluation.aimo_3_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "460823d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:18:11.959207Z",
     "iopub.status.busy": "2026-02-18T13:18:11.958943Z",
     "iopub.status.idle": "2026-02-18T13:18:11.963944Z",
     "shell.execute_reply": "2026-02-18T13:18:11.963569Z"
    },
    "papermill": {
     "duration": 0.010643,
     "end_time": "2026-02-18T13:18:11.964685",
     "exception": false,
     "start_time": "2026-02-18T13:18:11.954042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    system_prompt = (\n",
    "        'You are an elite mathematical problem solver with expertise at the International '\n",
    "        'Mathematical Olympiad (IMO) level. Your goal is to find the correct answer through '\n",
    "        'rigorous mathematical reasoning.\\n\\n'\n",
    "        \n",
    "        '# Problem-Solving Approach:\\n'\n",
    "        '1. UNDERSTAND: Carefully read and rephrase the problem in your own words. '\n",
    "        'Identify what is given, what needs to be found, and any constraints.\\n'\n",
    "        '2. EXPLORE: Consider multiple solution strategies. Think about relevant theorems, '\n",
    "        'techniques, patterns, or analogous problems. Don\\'t commit to one approach immediately.\\n'\n",
    "        '3. PLAN: Select the most promising approach and outline key steps before executing.\\n'\n",
    "        '4. EXECUTE: Work through your solution methodically. Show all reasoning steps clearly.\\n'\n",
    "        '5. VERIFY: Check your answer by substituting back, testing edge cases, or using '\n",
    "        'alternative methods. Ensure logical consistency throughout.\\n\\n'\n",
    "        \n",
    "        '# Mathematical Reasoning Principles:\\n'\n",
    "        '- Break complex problems into smaller, manageable sub-problems\\n'\n",
    "        '- Look for patterns, symmetries, and special cases that provide insight\\n'\n",
    "        '- Use concrete examples to build intuition before generalizing\\n'\n",
    "        '- Consider extreme cases and boundary conditions\\n'\n",
    "        '- If stuck, try working backwards from the desired result\\n'\n",
    "        '- Be willing to restart with a different approach if needed\\n\\n'\n",
    "        \n",
    "        '# Verification Requirements:\\n'\n",
    "        '- Cross-check arithmetic and algebraic manipulations\\n'\n",
    "        '- Verify that your solution satisfies all problem constraints\\n'\n",
    "        '- Test your answer with simple cases or special values when possible\\n'\n",
    "        '- Ensure dimensional consistency and reasonableness of the result\\n\\n'\n",
    "        \n",
    "        '# Output Format:\\n'\n",
    "        'The final answer must be a non-negative integer between 0 and 99999.\\n'\n",
    "        'Place your final numerical answer inside \\\\boxed{}, e.g., \\\\boxed{42}\\n\\n'\n",
    "        \n",
    "        'Think step-by-step and show your complete reasoning process. Quality of reasoning '\n",
    "        'is as important as the final answer.'\n",
    "    )\n",
    "    \n",
    "    tool_prompt = (\n",
    "        'Use this tool to execute Python code for:\\n'\n",
    "        '- Complex calculations that would be error-prone by hand\\n'\n",
    "        '- Numerical verification of analytical results\\n'\n",
    "        '- Generating examples or testing conjectures\\n'\n",
    "        '- Visualizing problem structure when helpful\\n'\n",
    "        '- Brute-force verification for small cases\\n\\n'\n",
    "        \n",
    "        'The environment is a stateful Jupyter notebook. Code persists between executions.\\n'\n",
    "        'Always use print() to display results. Write clear, well-commented code.\\n\\n'\n",
    "        \n",
    "        'Remember: Code should support your mathematical reasoning, not replace it. '\n",
    "        'Explain what you\\'re computing and why before running code.'\n",
    "    )\n",
    "    \n",
    "    preference_prompt = (\n",
    "        'You have access to `math`, `numpy`, and `sympy` for:\\n\\n'\n",
    "        \n",
    "        '# Symbolic Computation (sympy):\\n'\n",
    "        '- Algebraic manipulation and simplification\\n'\n",
    "        '- Solving equations and systems of equations\\n'\n",
    "        '- Symbolic differentiation and integration\\n'\n",
    "        '- Number theory functions (primes, divisors, modular arithmetic)\\n'\n",
    "        '- Polynomial operations and factorization\\n'\n",
    "        '- Working with mathematical expressions symbolically\\n\\n'\n",
    "        \n",
    "        '# Numerical Computation (numpy):\\n'\n",
    "        '- Array operations and linear algebra\\n'\n",
    "        '- Efficient numerical calculations for large datasets\\n'\n",
    "        '- Matrix operations and eigenvalue problems\\n'\n",
    "        '- Statistical computations\\n\\n'\n",
    "        \n",
    "        '# Mathematical Functions (math):\\n'\n",
    "        '- Standard mathematical functions (trig, log, exp)\\n'\n",
    "        '- Constants like pi and e\\n'\n",
    "        '- Basic operations for single values\\n\\n'\n",
    "        \n",
    "        'Best Practices:\\n'\n",
    "        '- Use sympy for exact symbolic answers when possible\\n'\n",
    "        '- Use numpy for numerical verification and large-scale computation\\n'\n",
    "        '- Combine symbolic and numerical approaches: derive symbolically, verify numerically\\n'\n",
    "        '- Document your computational strategy clearly\\n'\n",
    "        '- Validate computational results against known cases or theoretical bounds'\n",
    "    )\n",
    "    \n",
    "    served_model_name = 'gpt-oss'\n",
    "    model_path = '/kaggle/input/models/danielhanchen/gpt-oss-120b/transformers/default/1'\n",
    "    \n",
    "    kv_cache_dtype = 'fp8_e4m3'\n",
    "    dtype = 'auto'\n",
    "\n",
    "    high_problem_timeout = 900\n",
    "    base_problem_timeout = 300\n",
    "\n",
    "    notebook_limit = 17400\n",
    "    server_timeout = 180\n",
    "\n",
    "    session_timeout = 960\n",
    "    jupyter_timeout = 6\n",
    "    sandbox_timeout = 3\n",
    "\n",
    "    stream_interval = 200\n",
    "    context_tokens = 65536\n",
    "    buffer_tokens = 512\n",
    "    search_tokens = 32\n",
    "    top_logprobs = 5\n",
    "    batch_size = 256\n",
    "    early_stop = 4\n",
    "    attempts = 8\n",
    "    workers = 16\n",
    "    turns = 128\n",
    "    seed = 42\n",
    "\n",
    "    gpu_memory_utilization = 0.96\n",
    "    temperature = 1.0\n",
    "    # min_p = 0.02\n",
    "    # With higher temperature, slightly increase min_p to filter noise\n",
    "    min_p = 0.03  # Was 0.02\n",
    "    \n",
    "    # Or use top_p instead of/in addition to min_p\n",
    "    top_p = 0.95  # Nucleus sampling as another control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1b8fc36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:18:11.973299Z",
     "iopub.status.busy": "2026-02-18T13:18:11.973156Z",
     "iopub.status.idle": "2026-02-18T13:18:11.976784Z",
     "shell.execute_reply": "2026-02-18T13:18:11.976440Z"
    },
    "papermill": {
     "duration": 0.008694,
     "end_time": "2026-02-18T13:18:11.977501",
     "exception": false,
     "start_time": "2026-02-18T13:18:11.968807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a0fa67f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:18:11.986206Z",
     "iopub.status.busy": "2026-02-18T13:18:11.986071Z",
     "iopub.status.idle": "2026-02-18T13:18:11.989210Z",
     "shell.execute_reply": "2026-02-18T13:18:11.988873Z"
    },
    "papermill": {
     "duration": 0.008356,
     "end_time": "2026-02-18T13:18:11.989943",
     "exception": false,
     "start_time": "2026-02-18T13:18:11.981587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Template:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n",
    "\n",
    "        return (\n",
    "            SystemContent.new()\n",
    "            .with_model_identity(system_prompt)\n",
    "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
    "            .with_tools(tool_config)\n",
    "        )\n",
    "\n",
    "    def apply_chat_template(\n",
    "        self, \n",
    "        system_prompt: str, \n",
    "        user_prompt: str, \n",
    "        tool_config: ToolNamespaceConfig\n",
    "    ) -> list[Message]:\n",
    "\n",
    "        system_content = self.get_system_content(system_prompt, tool_config)        \n",
    "        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
    "\n",
    "        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n",
    "\n",
    "        return [system_message, user_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f57a4302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:18:11.998437Z",
     "iopub.status.busy": "2026-02-18T13:18:11.998302Z",
     "iopub.status.idle": "2026-02-18T13:18:12.007984Z",
     "shell.execute_reply": "2026-02-18T13:18:12.007617Z"
    },
    "papermill": {
     "duration": 0.014812,
     "end_time": "2026-02-18T13:18:12.008738",
     "exception": false,
     "start_time": "2026-02-18T13:18:11.993926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Sandbox:\n",
    "\n",
    "    _port_lock = threading.Lock()\n",
    "    _next_port = 50000\n",
    "\n",
    "    @classmethod\n",
    "    def _get_next_ports(cls, count: int = 5) -> list[int]:\n",
    "\n",
    "        with cls._port_lock:\n",
    "            ports = list(range(cls._next_port, cls._next_port + count))\n",
    "            cls._next_port += count\n",
    "\n",
    "            return ports\n",
    "\n",
    "    def __init__(self, timeout: float):\n",
    "\n",
    "        self._default_timeout = timeout\n",
    "        self._owns_kernel = False\n",
    "        self._client = None\n",
    "        self._km = None\n",
    "        \n",
    "        ports = self._get_next_ports(5)\n",
    "\n",
    "        env = os.environ.copy()\n",
    "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n",
    "        env['JUPYTER_PLATFORM_DIRS'] = '1'\n",
    "        env['PYTHONWARNINGS'] = 'ignore'\n",
    "        env['MPLBACKEND'] = 'Agg'\n",
    "\n",
    "        self._km = KernelManager()\n",
    "        self._km.shell_port = ports[0]\n",
    "        self._km.iopub_port = ports[1]\n",
    "        self._km.stdin_port = ports[2]\n",
    "        self._km.hb_port = ports[3]\n",
    "        self._km.control_port = ports[4]\n",
    "\n",
    "        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n",
    "\n",
    "        self._client = self._km.blocking_client()\n",
    "        self._client.start_channels()\n",
    "        self._client.wait_for_ready(timeout=self._default_timeout)\n",
    "        self._owns_kernel = True\n",
    "\n",
    "        self.execute(\n",
    "            'import math\\n'\n",
    "            'import numpy\\n'\n",
    "            'import sympy\\n'\n",
    "            'import itertools\\n'\n",
    "            'import collections\\n'\n",
    "            'import mpmath\\n'\n",
    "            'mpmath.mp.dps = 64\\n'\n",
    "        )\n",
    "\n",
    "    def _format_error(self, traceback: list[str]) -> str:\n",
    "\n",
    "        clean_lines = []\n",
    "\n",
    "        for frame in traceback:\n",
    "            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n",
    "\n",
    "            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n",
    "                continue\n",
    "\n",
    "            clean_lines.append(clean_frame)\n",
    "\n",
    "        return ''.join(clean_lines)\n",
    "\n",
    "    def execute(self, code: str, timeout: float | None = None) -> str:\n",
    "\n",
    "        client = self._client\n",
    "        effective_timeout = timeout or self._default_timeout\n",
    "        \n",
    "        msg_id = client.execute(\n",
    "            code, \n",
    "            store_history=True, \n",
    "            allow_stdin=False, \n",
    "            stop_on_error=False\n",
    "        )\n",
    "\n",
    "        stdout_parts = []\n",
    "        stderr_parts = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            if elapsed > effective_timeout:\n",
    "                self._km.interrupt_kernel()\n",
    "\n",
    "                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n",
    "\n",
    "            try:\n",
    "                msg = client.get_iopub_msg(timeout=1.0)\n",
    "\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "\n",
    "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
    "                continue\n",
    "\n",
    "            msg_type = msg.get('msg_type')\n",
    "            content = msg.get('content', {})\n",
    "\n",
    "            if msg_type == 'stream':\n",
    "                text = content.get('text', '')\n",
    "\n",
    "                if content.get('name') == 'stdout':\n",
    "                    stdout_parts.append(text)\n",
    "\n",
    "                else:\n",
    "                    stderr_parts.append(text)\n",
    "\n",
    "            elif msg_type == 'error':\n",
    "                traceback_list = content.get('traceback', [])\n",
    "\n",
    "                stderr_parts.append(self._format_error(traceback_list))\n",
    "\n",
    "            elif msg_type in {'execute_result', 'display_data'}:\n",
    "                data = content.get('data', {})\n",
    "                text = data.get('text/plain')\n",
    "\n",
    "                if text:\n",
    "                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n",
    "\n",
    "            elif msg_type == 'status':\n",
    "                if content.get('execution_state') == 'idle':\n",
    "                    break\n",
    "\n",
    "        stdout = ''.join(stdout_parts)\n",
    "        stderr = ''.join(stderr_parts)\n",
    "\n",
    "        if stderr:\n",
    "            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n",
    "\n",
    "        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n",
    "\n",
    "    def close(self):\n",
    "\n",
    "        with contextlib.suppress(Exception):\n",
    "            if self._client:\n",
    "                self._client.stop_channels()\n",
    "\n",
    "        if self._owns_kernel and self._km is not None:\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.shutdown_kernel(now=True)\n",
    "\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.cleanup_resources()\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.execute(\n",
    "            '%reset -f\\n'\n",
    "            'import math\\n'\n",
    "            'import numpy\\n'\n",
    "            'import sympy\\n'\n",
    "            'import itertools\\n'\n",
    "            'import collections\\n'\n",
    "            'import mpmath\\n'\n",
    "            'mpmath.mp.dps = 64\\n'\n",
    "        )\n",
    "\n",
    "    def __del__(self):\n",
    "\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f460e29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:18:12.017156Z",
     "iopub.status.busy": "2026-02-18T13:18:12.017028Z",
     "iopub.status.idle": "2026-02-18T13:18:12.022475Z",
     "shell.execute_reply": "2026-02-18T13:18:12.022118Z"
    },
    "papermill": {
     "duration": 0.010559,
     "end_time": "2026-02-18T13:18:12.023167",
     "exception": false,
     "start_time": "2026-02-18T13:18:12.012608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Tool:\n",
    "\n",
    "    def __init__(self, local_jupyter_timeout: float, tool_prompt: str, sandbox=None):\n",
    "\n",
    "        self._local_jupyter_timeout = local_jupyter_timeout\n",
    "        self._tool_prompt = tool_prompt\n",
    "        self._jupyter_session = sandbox\n",
    "        \n",
    "        self._owns_session = sandbox is None\n",
    "        \n",
    "        self._execution_lock = threading.Lock()\n",
    "        self._init_lock = threading.Lock()\n",
    "\n",
    "    def _ensure_session(self):\n",
    "\n",
    "        if self._jupyter_session is None:\n",
    "            with self._init_lock:\n",
    "                if self._jupyter_session is None:\n",
    "                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n",
    "\n",
    "    def _ensure_last_print(self, code: str) -> str:\n",
    "\n",
    "        lines = code.strip().split('\\n')\n",
    "\n",
    "        if not lines:\n",
    "            return code\n",
    "\n",
    "        last_line = lines[-1].strip()\n",
    "\n",
    "        if 'print' in last_line or 'import' in last_line:\n",
    "            return code\n",
    "\n",
    "        if not last_line:\n",
    "            return code\n",
    "\n",
    "        if last_line.startswith('#'):\n",
    "            return code\n",
    "\n",
    "        lines[-1] = 'print(' + last_line + ')'\n",
    "\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    @property\n",
    "    def instruction(self) -> str:\n",
    "\n",
    "        return self._tool_prompt\n",
    "\n",
    "    @property\n",
    "    def tool_config(self) -> ToolNamespaceConfig:\n",
    "\n",
    "        return ToolNamespaceConfig(\n",
    "            name='python', \n",
    "            description=self.instruction, \n",
    "            tools=[]\n",
    "        )\n",
    "\n",
    "    def _make_response(self, output: str, channel: str | None = None) -> Message:\n",
    "\n",
    "        content = TextContent(text=output)\n",
    "        author = Author(role=Role.TOOL, name='python')\n",
    "        message = Message(author=author, content=[content]).with_recipient('assistant')\n",
    "\n",
    "        if channel:\n",
    "            message = message.with_channel(channel)\n",
    "\n",
    "        return message\n",
    "\n",
    "    def process_sync_plus(self, message: Message) -> list[Message]:\n",
    "\n",
    "        self._ensure_session()\n",
    "        raw_script = message.content[0].text\n",
    "        final_script = self._ensure_last_print(raw_script)\n",
    "\n",
    "        with self._execution_lock:\n",
    "            try:\n",
    "                output = self._jupyter_session.execute(final_script)\n",
    "\n",
    "            except TimeoutError as exc:\n",
    "                output = f'[ERROR] {exc}'\n",
    "\n",
    "        return [self._make_response(output, channel=message.channel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c79963f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:18:12.032358Z",
     "iopub.status.busy": "2026-02-18T13:18:12.032223Z",
     "iopub.status.idle": "2026-02-18T13:18:12.055223Z",
     "shell.execute_reply": "2026-02-18T13:18:12.054675Z"
    },
    "papermill": {
     "duration": 0.028579,
     "end_time": "2026-02-18T13:18:12.055993",
     "exception": false,
     "start_time": "2026-02-18T13:18:12.027414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Solver:\n",
    "\n",
    "    def __init__(self, cfg, port: int = 8000):\n",
    "    \n",
    "        self.cfg = cfg\n",
    "        self.port = port\n",
    "        self.base_url = f'http://0.0.0.0:{port}/v1'\n",
    "        self.api_key = 'sk-local'\n",
    "        self.template = AIMO3Template()\n",
    "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
    "    \n",
    "        self._preload_model_weights()\n",
    "        \n",
    "        self.server_process = self._start_server()\n",
    "    \n",
    "        self.client = OpenAI(\n",
    "            base_url=self.base_url, \n",
    "            api_key=self.api_key, \n",
    "            timeout=self.cfg.session_timeout\n",
    "        )\n",
    "    \n",
    "        self._wait_for_server()\n",
    "        self._initialize_kernels()\n",
    "    \n",
    "        self.notebook_start_time = time.time()\n",
    "        self.problems_remaining = 50\n",
    "    \n",
    "    def _preload_model_weights(self) -> None:\n",
    "    \n",
    "        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        files_to_load = []\n",
    "        total_size = 0\n",
    "    \n",
    "        for root, _, files in os.walk(self.cfg.model_path):\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "    \n",
    "                if os.path.isfile(file_path):\n",
    "                    files_to_load.append(file_path)\n",
    "                    total_size += os.path.getsize(file_path)\n",
    "    \n",
    "        def _read_file(path: str) -> None:\n",
    "    \n",
    "            with open(path, 'rb') as file_object:\n",
    "                while file_object.read(1024 * 1024 * 1024):\n",
    "                    pass\n",
    "    \n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
    "            list(executor.map(_read_file, files_to_load))\n",
    "    \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n",
    "    \n",
    "    def _start_server(self) -> subprocess.Popen:\n",
    "    \n",
    "        cmd = [\n",
    "            sys.executable, \n",
    "            '-m', \n",
    "            'vllm.entrypoints.openai.api_server', \n",
    "            '--seed', \n",
    "            str(self.cfg.seed), \n",
    "            '--model', \n",
    "            self.cfg.model_path, \n",
    "            '--served-model-name', \n",
    "            self.cfg.served_model_name, \n",
    "            '--tensor-parallel-size', \n",
    "            '1', \n",
    "            '--max-num-seqs', \n",
    "            str(self.cfg.batch_size), \n",
    "            '--gpu-memory-utilization', \n",
    "            str(self.cfg.gpu_memory_utilization), \n",
    "            '--host', \n",
    "            '0.0.0.0', \n",
    "            '--port', \n",
    "            str(self.port), \n",
    "            '--dtype', \n",
    "            self.cfg.dtype, \n",
    "            '--kv-cache-dtype', \n",
    "            self.cfg.kv_cache_dtype, \n",
    "            '--max-model-len', \n",
    "            str(self.cfg.context_tokens), \n",
    "            '--stream-interval', \n",
    "            str(self.cfg.stream_interval), \n",
    "            '--async-scheduling', \n",
    "            '--disable-log-stats', \n",
    "            '--enable-prefix-caching'\n",
    "        ]\n",
    "    \n",
    "        self.log_file = open('vllm_server.log', 'w')\n",
    "    \n",
    "        return subprocess.Popen(\n",
    "            cmd, \n",
    "            stdout=self.log_file, \n",
    "            stderr=subprocess.STDOUT, \n",
    "            start_new_session=True\n",
    "        )\n",
    "    \n",
    "    def _wait_for_server(self):\n",
    "    \n",
    "        print('Waiting for vLLM server...')\n",
    "        start_time = time.time()\n",
    "    \n",
    "        for _ in range(self.cfg.server_timeout):\n",
    "            return_code = self.server_process.poll()\n",
    "    \n",
    "            if return_code is not None:\n",
    "                self.log_file.flush()\n",
    "    \n",
    "                with open('vllm_server.log', 'r') as log_file:\n",
    "                    logs = log_file.read()\n",
    "    \n",
    "                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n",
    "    \n",
    "            try:\n",
    "                self.client.models.list()\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n",
    "    \n",
    "                return\n",
    "    \n",
    "            except Exception:\n",
    "                time.sleep(1)\n",
    "    \n",
    "        raise RuntimeError('Server failed to start (timeout).\\n')\n",
    "    \n",
    "    def _initialize_kernels(self) -> None:\n",
    "    \n",
    "        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n",
    "        start_time = time.time()\n",
    "    \n",
    "        self.sandbox_pool = queue.Queue()\n",
    "    \n",
    "        def _create_sandbox():\n",
    "            \n",
    "            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n",
    "    \n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
    "            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n",
    "    \n",
    "            for future in as_completed(futures):\n",
    "                self.sandbox_pool.put(future.result())\n",
    "    \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n",
    "    \n",
    "    def _scan_for_answer(self, text: str) -> int | None:\n",
    "        \n",
    "        pattern = r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}'\n",
    "        matches = re.findall(pattern, text)\n",
    "    \n",
    "        if matches:\n",
    "            try:\n",
    "                clean_value = matches[-1].replace(',', '')\n",
    "                value = int(clean_value)\n",
    "    \n",
    "                if 0 <= value <= 99999:\n",
    "                    return value\n",
    "    \n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "        pattern = r'final\\s+answer\\s+is\\s*([0-9,]+)'\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    \n",
    "        if matches:\n",
    "            try:\n",
    "                clean_value = matches[-1].replace(',', '')\n",
    "                value = int(clean_value)\n",
    "    \n",
    "                if 0 <= value <= 99999:\n",
    "                    return value\n",
    "    \n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    def _compute_mean_entropy(self, logprobs_buffer: list) -> float:\n",
    "    \n",
    "        if not logprobs_buffer:\n",
    "            return float('inf')\n",
    "    \n",
    "        total_entropy = 0.0\n",
    "        token_count = 0\n",
    "    \n",
    "        for top_logprobs_dict in logprobs_buffer:\n",
    "            \n",
    "            if not isinstance(top_logprobs_dict, dict):\n",
    "                continue\n",
    "            \n",
    "            if not top_logprobs_dict:\n",
    "                continue\n",
    "            \n",
    "            token_entropy = 0.0\n",
    "            \n",
    "            for token_str, log_prob in top_logprobs_dict.items():\n",
    "                prob = math.exp(log_prob)\n",
    "                \n",
    "                if prob > 0:\n",
    "                    token_entropy -= prob * math.log2(prob)\n",
    "            \n",
    "            total_entropy += token_entropy\n",
    "            token_count += 1\n",
    "    \n",
    "        if token_count == 0:\n",
    "            return float('inf')\n",
    "    \n",
    "        return total_entropy / token_count\n",
    "    \n",
    "    def _process_attempt(\n",
    "        self, \n",
    "        problem: str, \n",
    "        system_prompt: str, \n",
    "        attempt_index: int, \n",
    "        stop_event: threading.Event, \n",
    "        deadline: float\n",
    "    ) -> dict:\n",
    "    \n",
    "        if stop_event.is_set() or time.time() > deadline:\n",
    "            return {\n",
    "                'Attempt': attempt_index + 1, \n",
    "                'Answer': None, \n",
    "                'Python Calls': 0, \n",
    "                'Python Errors': 0, \n",
    "                'Response Length': 0, \n",
    "                'Entropy': float('inf')\n",
    "            }\n",
    "    \n",
    "        local_tool = None\n",
    "        sandbox = None\n",
    "        python_calls = 0\n",
    "        python_errors = 0\n",
    "        total_tokens = 0\n",
    "        final_answer = None\n",
    "        \n",
    "        logprobs_buffer = []\n",
    "    \n",
    "        attempt_seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n",
    "    \n",
    "        try:\n",
    "            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n",
    "    \n",
    "            local_tool = AIMO3Tool(\n",
    "                local_jupyter_timeout=self.cfg.jupyter_timeout, \n",
    "                tool_prompt=self.cfg.tool_prompt, \n",
    "                sandbox=sandbox\n",
    "            )\n",
    "    \n",
    "            encoding = self.encoding\n",
    "            messages = self.template.apply_chat_template(\n",
    "                system_prompt, \n",
    "                problem, \n",
    "                local_tool.tool_config\n",
    "            )\n",
    "    \n",
    "            conversation = Conversation.from_messages(messages)\n",
    "    \n",
    "            for _ in range(self.cfg.turns):\n",
    "                if stop_event.is_set() or time.time() > deadline:\n",
    "                    break\n",
    "    \n",
    "                prompt_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
    "                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n",
    "    \n",
    "                if max_tokens < self.cfg.buffer_tokens:\n",
    "                    break\n",
    "    \n",
    "                stream = self.client.completions.create(\n",
    "                    model=self.cfg.served_model_name, \n",
    "                    temperature=self.cfg.temperature, \n",
    "                    logprobs=self.cfg.top_logprobs, \n",
    "                    max_tokens=max_tokens, \n",
    "                    prompt=prompt_ids, \n",
    "                    seed=attempt_seed, \n",
    "                    stream=True, \n",
    "                    extra_body={\n",
    "                        'min_p': self.cfg.min_p, \n",
    "                        'top_p': self.cfg.top_p, \n",
    "                        'stop_token_ids': self.stop_token_ids, \n",
    "                        'return_token_ids': True\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "                try:\n",
    "                    token_buffer = []\n",
    "                    text_chunks = []\n",
    "    \n",
    "                    for chunk in stream:\n",
    "                        if stop_event.is_set() or time.time() > deadline:\n",
    "                            break\n",
    "    \n",
    "                        new_tokens = chunk.choices[0].token_ids\n",
    "                        new_text = chunk.choices[0].text\n",
    "    \n",
    "                        if new_tokens:\n",
    "                            token_buffer.extend(new_tokens)\n",
    "                            total_tokens += len(new_tokens)\n",
    "                            text_chunks.append(new_text)\n",
    "                            \n",
    "                            chunk_logprobs = chunk.choices[0].logprobs\n",
    "                            \n",
    "                            if chunk_logprobs is not None:\n",
    "                                if chunk_logprobs.top_logprobs:\n",
    "                                    logprobs_buffer.extend(chunk_logprobs.top_logprobs)\n",
    "    \n",
    "                        if '}' in new_text:\n",
    "                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n",
    "                            answer = self._scan_for_answer(search_text)\n",
    "    \n",
    "                            if answer is not None:\n",
    "                                final_answer = answer\n",
    "                                break\n",
    "    \n",
    "                finally:\n",
    "                    stream.close()\n",
    "    \n",
    "                if final_answer is not None:\n",
    "                    break\n",
    "    \n",
    "                if not token_buffer:\n",
    "                    break\n",
    "    \n",
    "                new_messages = encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n",
    "                conversation.messages.extend(new_messages)\n",
    "                last_message = new_messages[-1]\n",
    "    \n",
    "                if last_message.channel == 'final':\n",
    "                    answer_text = last_message.content[0].text\n",
    "                    final_answer = self._scan_for_answer(answer_text)\n",
    "                    break\n",
    "    \n",
    "                if last_message.recipient == 'python':\n",
    "                    python_calls += 1\n",
    "                    tool_responses = local_tool.process_sync_plus(last_message)\n",
    "    \n",
    "                    response_text = tool_responses[0].content[0].text\n",
    "    \n",
    "                    if response_text.startswith('[ERROR]') or 'Traceback' in response_text or 'Error:' in response_text:\n",
    "                        python_errors += 1\n",
    "    \n",
    "                    conversation.messages.extend(tool_responses)\n",
    "    \n",
    "        except Exception as exc:\n",
    "            python_errors += 1\n",
    "    \n",
    "        finally:\n",
    "            if sandbox is not None:\n",
    "                sandbox.reset()\n",
    "                self.sandbox_pool.put(sandbox)\n",
    "    \n",
    "        mean_entropy = self._compute_mean_entropy(logprobs_buffer)\n",
    "    \n",
    "        return {\n",
    "            'Attempt': attempt_index + 1, \n",
    "            'Response Length': total_tokens, \n",
    "            'Python Calls': python_calls, \n",
    "            'Python Errors': python_errors, \n",
    "            'Entropy': mean_entropy, \n",
    "            'Answer': final_answer\n",
    "        }\n",
    "    \n",
    "    def _select_answer(self, detailed_results: list) -> int:\n",
    "\n",
    "        answer_weights = defaultdict(float)\n",
    "        answer_votes = defaultdict(int)\n",
    "\n",
    "        for result in detailed_results:\n",
    "            answer = result['Answer']\n",
    "            entropy = result['Entropy']\n",
    "            \n",
    "            if answer is not None:\n",
    "                weight = 1.0 / max(entropy, 1e-9)\n",
    "                \n",
    "                answer_weights[answer] += weight\n",
    "                answer_votes[answer] += 1\n",
    "\n",
    "        scored_answers = []\n",
    "\n",
    "        for answer, total_weight in answer_weights.items():\n",
    "            scored_answers.append({\n",
    "                'answer': answer, \n",
    "                'votes': answer_votes[answer], \n",
    "                'score': total_weight\n",
    "            })\n",
    "\n",
    "        scored_answers.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        vote_data = []\n",
    "\n",
    "        for item in scored_answers:\n",
    "            vote_data.append((\n",
    "                item['answer'], \n",
    "                item['votes'], \n",
    "                item['score']\n",
    "            ))\n",
    "\n",
    "        vote_dataframe = pd.DataFrame(\n",
    "            vote_data, \n",
    "            columns=['Answer', 'Votes', 'Score']\n",
    "        )\n",
    "\n",
    "        vote_dataframe = vote_dataframe.round({'Score': 3})\n",
    "        display(vote_dataframe)\n",
    "        \n",
    "        if not scored_answers:\n",
    "            print('\\nFinal Answer: 0\\n')\n",
    "            return 0\n",
    "\n",
    "        final_answer = scored_answers[0]['answer']    \n",
    "        print(f'\\nFinal Answer: {final_answer}\\n')\n",
    "\n",
    "        return final_answer\n",
    "    \n",
    "    def solve_problem(self, problem: str) -> int:\n",
    "    \n",
    "        print(f'\\nProblem: {problem}\\n')\n",
    "        \n",
    "        user_input = f'{problem} {self.cfg.preference_prompt}'\n",
    "    \n",
    "        elapsed_global = time.time() - self.notebook_start_time\n",
    "        time_left = self.cfg.notebook_limit - elapsed_global\n",
    "        problems_left_others = max(0, self.problems_remaining - 1)\n",
    "        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n",
    "    \n",
    "        budget = time_left - reserved_time\n",
    "        budget = min(budget, self.cfg.high_problem_timeout)\n",
    "        budget = max(budget, self.cfg.base_problem_timeout)\n",
    "    \n",
    "        deadline = time.time() + budget\n",
    "    \n",
    "        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n",
    "    \n",
    "        tasks = []\n",
    "    \n",
    "        for attempt_index in range(self.cfg.attempts):\n",
    "            tasks.append((self.cfg.system_prompt, attempt_index))\n",
    "    \n",
    "        detailed_results = []\n",
    "        valid_answers = []\n",
    "    \n",
    "        stop_event = threading.Event()\n",
    "    \n",
    "        executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n",
    "    \n",
    "        try:\n",
    "            futures = []\n",
    "    \n",
    "            for (system_prompt, attempt_index) in tasks:\n",
    "                future = executor.submit(\n",
    "                    self._process_attempt, \n",
    "                    user_input, \n",
    "                    system_prompt, \n",
    "                    attempt_index, \n",
    "                    stop_event, \n",
    "                    deadline\n",
    "                )\n",
    "    \n",
    "                futures.append(future)\n",
    "    \n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    detailed_results.append(result)\n",
    "    \n",
    "                    if result['Answer'] is not None:\n",
    "                        valid_answers.append(result['Answer'])\n",
    "    \n",
    "                    counts = Counter(valid_answers).most_common(1)\n",
    "    \n",
    "                    if counts and counts[0][1] >= self.cfg.early_stop:\n",
    "                        stop_event.set()\n",
    "    \n",
    "                        for f in futures:\n",
    "                            f.cancel()\n",
    "    \n",
    "                        break\n",
    "    \n",
    "                except Exception as exc:\n",
    "                    print(f'Future failed: {exc}')\n",
    "                    continue\n",
    "    \n",
    "        finally:\n",
    "            stop_event.set()\n",
    "            executor.shutdown(wait=True, cancel_futures=True)\n",
    "            \n",
    "            self.problems_remaining = max(0, self.problems_remaining - 1)\n",
    "    \n",
    "        if detailed_results:\n",
    "            results_dataframe = pd.DataFrame(detailed_results)\n",
    "            results_dataframe['Entropy'] = results_dataframe['Entropy'].round(3)\n",
    "            results_dataframe['Answer'] = results_dataframe['Answer'].astype('Int64')\n",
    "            \n",
    "            display(results_dataframe)\n",
    "    \n",
    "        if not valid_answers:\n",
    "            print('\\nResult: 0\\n')\n",
    "    \n",
    "            return 0\n",
    "    \n",
    "        return self._select_answer(detailed_results)\n",
    "    \n",
    "    def __del__(self):\n",
    "    \n",
    "        if hasattr(self, 'server_process'):\n",
    "            self.server_process.terminate()\n",
    "            self.server_process.wait()\n",
    "    \n",
    "        if hasattr(self, 'log_file'):\n",
    "            self.log_file.close()\n",
    "    \n",
    "        if hasattr(self, 'sandbox_pool'):\n",
    "            while not self.sandbox_pool.empty():\n",
    "                try:\n",
    "                    sb = self.sandbox_pool.get_nowait()\n",
    "                    sb.close()\n",
    "    \n",
    "                except Exception:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7016cf0d",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-02-18T13:18:12.064650Z",
     "iopub.status.busy": "2026-02-18T13:18:12.064514Z",
     "iopub.status.idle": "2026-02-18T13:21:41.017104Z",
     "shell.execute_reply": "2026-02-18T13:21:41.016604Z"
    },
    "papermill": {
     "duration": 208.957975,
     "end_time": "2026-02-18T13:21:41.017976",
     "exception": false,
     "start_time": "2026-02-18T13:18:12.060001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights from /kaggle/input/models/danielhanchen/gpt-oss-120b/transformers/default/1 into OS Page Cache...\n",
      "Processed 26 files (65.28 GB) in 80.48 seconds.\n",
      "\n",
      "Waiting for vLLM server...\n",
      "Server is ready (took 125.25 seconds).\n",
      "\n",
      "Initializing 16 persistent Jupyter kernels...\n",
      "Kernels initialized in 2.87 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "solver = AIMO3Solver(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e734472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T13:21:41.027685Z",
     "iopub.status.busy": "2026-02-18T13:21:41.027480Z",
     "iopub.status.idle": "2026-02-18T13:21:41.030998Z",
     "shell.execute_reply": "2026-02-18T13:21:41.030533Z"
    },
    "papermill": {
     "duration": 0.009236,
     "end_time": "2026-02-18T13:21:41.031778",
     "exception": false,
     "start_time": "2026-02-18T13:21:41.022542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
    "    \n",
    "    id_value = id_.item(0)\n",
    "    question_text = question.item(0)\n",
    "    \n",
    "    gc.disable()\n",
    "    \n",
    "    final_answer = solver.solve_problem(question_text)\n",
    "    \n",
    "    gc.enable()\n",
    "    gc.collect()\n",
    "    \n",
    "    return pl.DataFrame({'id': id_value, 'answer': final_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4921bf43",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-02-18T13:21:41.041109Z",
     "iopub.status.busy": "2026-02-18T13:21:41.040947Z",
     "iopub.status.idle": "2026-02-18T13:22:02.437561Z",
     "shell.execute_reply": "2026-02-18T13:22:02.437092Z"
    },
    "papermill": {
     "duration": 21.402302,
     "end_time": "2026-02-18T13:22:02.438445",
     "exception": false,
     "start_time": "2026-02-18T13:21:41.036143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem: What is $1-1$?\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1771421801.44\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer\n",
       "0        8              108             0              0    0.545       0\n",
       "1        1              116             0              0    0.766       0\n",
       "2        2              140             0              0    0.708       0\n",
       "3        7              151             0              0    0.800       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0       0      4  5.803"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 0\n",
      "\n",
      "\n",
      "Problem: Solve $4+x=4$ for $x$.\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1771421818.26\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer\n",
       "0        6              111             0              0    0.494       0\n",
       "1        3              133             0              0    0.607       0\n",
       "2        8              178             0              0    0.619       0\n",
       "3        7              180             0              0    0.705       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0       0      4   6.71"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 0\n",
      "\n",
      "\n",
      "Problem: What is $0\\times10$?\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1771421820.35\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer\n",
       "0        8              121             0              0    0.570       0\n",
       "1        2              146             0              0    0.661       0\n",
       "2        4              153             0              0    0.734       0\n",
       "3        7              170             0              0    0.850       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0       0      4  5.808"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "    \n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "isSourceIdPinned": false,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "sourceId": 289055161,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 422384,
     "modelInstanceId": 404485,
     "sourceId": 510391,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 499.477537,
   "end_time": "2026-02-18T13:22:04.363398",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-18T13:13:44.885861",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
